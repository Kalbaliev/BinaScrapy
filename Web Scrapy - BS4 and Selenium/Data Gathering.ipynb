{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Data Scraper ( BS4 and Selenium Combine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from time import time,sleep\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaAZScraper():\n",
    "    \n",
    "    \"\"\" Please change chromedriver.exe path when pull project to computer\"\"\"\n",
    "    \n",
    "    headers={\"User-Agent\":\"Chrome/100.0.4896.12\"}\n",
    "    paths={\n",
    "    'driver_path':'C:/Users/Ali Kalbaliyev/Desktop/PMD/chromedriver.exe', #changeableside\n",
    "    'number_xpath':'//*[@id=\"show-phones\"]',\n",
    "    'sale_xpath':'//*[@id=\"js-main-col\"]/div[2]/div[1]/div/a[3]',\n",
    "    'agency_xpath':'//*[@id=\"js-main-col\"]/div[2]/div[1]/div/a[3]'\n",
    "    }\n",
    "    \n",
    "    def __init__(self,url):\n",
    "        self.url=url\n",
    "        self.announcement_links=[]\n",
    "        self.data=[]\n",
    "\n",
    "\n",
    "    def getUrlsFromAgency(self):\n",
    "        start=time()\n",
    "        page=requests.get(self.url,headers=self.headers)\n",
    "        print(page)\n",
    "        soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "        agencies_urls=[\"https://bina.az\"+el.get('href') for el in soup.findAll('a',class_='agencies-i')]\n",
    "        print(\"End of Scraping\")\n",
    "        return list(set(agencies_urls))\n",
    "\n",
    "    def parseDataAgency(self,url):\n",
    "        d=dict()\n",
    "        page=requests.get(url,headers=self.headers)\n",
    "        soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "        if soup.findAll('a',class_='agency-filters-i'):\n",
    "            soup=self.getSourceFromSelenium(url,self.paths['agency_xpath'])\n",
    "            d[\"title\"]=soup.find('h1',class_='agency--title').text\n",
    "            d[\"countTotal\"]=re.search(r'\\d+',soup.find('a',class_='agency--offers-count').text).group(0)\n",
    "            d['building']=\"Yeni tikililərin satışı\"\n",
    "            d[\"countNewBuilding\"]=0\n",
    "            building=soup.find('div',class_='agencies-row--title')\n",
    "            if building:\n",
    "                if building.text==\"Yeni tikililərin satışı\":\n",
    "                    d['building']=building.text\n",
    "                    countNewBuilding=soup.find('a',class_='agencies-row--show-all js-agencies-row-show-group')\n",
    "                    if countNewBuilding:\n",
    "                        d[\"countNewBuilding\"]=re.search(r'\\d+',countNewBuilding.text).group(0)\n",
    "                    else:\n",
    "                        print(\"bura girdi\",len(soup.find('div',class_='agencies-row--list items_list').findAll('div',class_='items-i')))\n",
    "                        d[\"countNewBuilding\"]=len(soup.find('div',class_='agencies-row--list items_list').findAll('div',class_='items-i'))\n",
    "      \n",
    "        return d\n",
    "    def getUrlsFromNewBuildings(self):\n",
    "        start=time()\n",
    "        \n",
    "        page=requests.get(self.url,headers=self.headers)\n",
    "        print(page)\n",
    "        soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "        announcements=soup.findAll('div',class_=\"items_list\")\n",
    "        if announcements:\n",
    "            link_tags=list(set(announcements[2].findAll(\"a\",class_=\"item_link\")))\n",
    "            for link in link_tags:\n",
    "                if \"items\" in link.get(\"href\"):\n",
    "                    self.announcement_links.append(\"https://bina.az\"+link.get(\"href\"))\n",
    "            next_button_url=soup.find('a',attrs={\"rel\":\"next\"})\n",
    "            if next_button_url:\n",
    "                next_button_url=\"https://bina.az\"+next_button_url.get(\"href\")\n",
    "                print(f'Second: {time()-start}\\nTotal Links\\' size:',len(self.announcement_links),\"\\nScraped Url:\",next_button_url,\"\\n\")\n",
    "                return self.getUrlsFromNewBuildings(next_button_url)\n",
    "        print(\"End of Scraping\")\n",
    "        return list(set(self.announcement_links))\n",
    "\n",
    "    def saveLinkData(self,data,name):\n",
    "        pickle.dump(data, open(f\"{name}.pkl\", \"wb\"))\n",
    "        \n",
    "    #Selenium Side\n",
    "    def seleniumActivate(self,url,xpath_value):\n",
    "        self.driver = webdriver.Chrome(self.paths['driver_path'])\n",
    "        self.driver.get(url)\n",
    "        next_button = self.driver.find_element(by='xpath',value=xpath_value)\n",
    "        next_button.click()\n",
    "        sleep(1)\n",
    "        return self.driver\n",
    "    \n",
    "    def getSourceFromSelenium(self,url,xpath_value):\n",
    "        self.seleniumActivate(url,xpath_value)\n",
    "        self.soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        self.driver.close()\n",
    "        return self.soup\n",
    "    \n",
    "    def parseData(self,url):\n",
    "        # Agency \n",
    "        src=self.getSourceFromSelenium(url,self.paths['number_xpath'])\n",
    "        table={el.findAll('td')[0].text:el.findAll('td')[1].text for el in src.find('table',class_='parameters').findAll('tr')}\n",
    "        agency_name=src.find('h1',class_='agency--title')\n",
    "        if agency_name:\n",
    "            table['Agency Name']=agency_name.text\n",
    "            # agency_url=\"https://bina.az\"+src.find('a',class_='agency--offers-count').get('href')\n",
    "            # table['Count of New buildings of Agency']=re.search(r'\\d+',getSourceFromSelenium(agency_url,paths['sale_xpath']).find('a',class_='agencies-row--show-all js-agencies-row-show-group').text).group(0)\n",
    "\n",
    "        table['Price']=src.find('span',class_='price-val').text\n",
    "        table['Currency']=src.find('span',class_='price-cur').text\n",
    "        table['Unit Price']=src.find('div',class_='unit-price').text\n",
    "\n",
    "        src=src.find('div',class_='item_show_content')\n",
    "\n",
    "        table['Description']=src.find('article').text\n",
    "        #Map And Locations\n",
    "        table['Address']=src.find('div',class_='map_address').text\n",
    "        table['Locations']=[el.text for el in src.find('ul',class_='locations').findAll('li')]\n",
    "\n",
    "        map_info=src.find('div',id='item_map')\n",
    "        table['Latitute']=map_info.get('data-lat')\n",
    "        table['Longitude']=map_info.get('data-lng')\n",
    "\n",
    "        #Seller\n",
    "        table['Seller Name']=src.find('div',class_='name').find(text=True)\n",
    "        table['Ownership']=src.find('span',class_='ownership').text\n",
    "        table['Phone Numbers']=[el.text for el in src.find('div',class_='js-phones').findAll('li')]\n",
    "\n",
    "        #Announce Info\n",
    "        item_info=src.find('div',class_='item_info').findAll('p')\n",
    "\n",
    "        table['Announcement ID']=item_info[0].text\n",
    "        table['Views']=item_info[1].text\n",
    "        table['Update Time']=item_info[2].text\n",
    "        return table\n",
    "    \n",
    "    def saveDataSet(self,name):\n",
    "        df=pd.DataFrame(self.data)\n",
    "        df.to_json(f'{name}.json',orient=\"records\",force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaSC=BinaAZScraper(\"https://bina.az/alqi-satqi/menziller/yeni-tikili\")\n",
    "agencySC=BinaAZScraper(\"https://bina.az/agentlikler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Announcements' Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkdata=binaSC.getUrlsFromNewBuildings()\n",
    "binaSC.saveLinkData(linkdata,'announcement_links')\n",
    "\n",
    "#Agency\n",
    "linkdata=agencySC.getUrlsFromAgency()\n",
    "agencySC.saveLinkData(linkdata,'agency_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Announcements' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=pickle.load(open(\"announcement_links.pkl\",'rb'))\n",
    "counter=1\n",
    "for url in urls:\n",
    "    start=time()\n",
    "    binaSC.data.append(binaSC.parseData(url))\n",
    "    print(f'Second: {time()-start}\\nAnnouncement:{counter}\\nScraped Url:{url}\\n')\n",
    "    counter+=1\n",
    "    \n",
    "binaSC.saveDataSet('binadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=pickle.load(open(\"agency_links.pkl\",'rb'))\n",
    "counter=1\n",
    "for url in urls:\n",
    "    start=time()\n",
    "    agencySC.data.append(agencySC.parseDataAgency(url))\n",
    "    print(f'Second: {time()-start}\\nAnnouncement:{counter}\\nScraped Url:{url}\\n')\n",
    "    counter+=1\n",
    "    \n",
    "agencySC.saveDataSet('agencydata')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
